{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "main = os.chdir(os.path.dirname(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from model import model_architecture, output_results, utils\n",
    "from pycox.models import CoxTime\n",
    "from pycox.models.cox_time import MLPVanillaCoxTime\n",
    "from sksurv.util import Surv as skSurv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Simulation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the censoring rate of the simulated data (it can be either 0.2  or 0.6). Data is simulated by the random function generator introduced by Friedman et al. (2001). \n",
    "Data is normalized (with mean and std from train set for train and test set) and splitted into training and test set (df_train and df_test are subsets of df_sim). The same training and test set are used for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"CoxTime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_sim = \"data/simulations/\"+str(rate)+\"/\"\n",
    "\n",
    "df_sim = pd.read_csv(dir_sim+'simdata.csv')\n",
    "df_train = pd.read_csv(dir_sim+'sim_train.csv')\n",
    "df_test = pd.read_csv(dir_sim+'sim_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, duration_test, event_test, labtrans = utils.prepare_data(df_train, df_test,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained using the best parameters determined by a 5 folds cross validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name==\"DeepHit\":\n",
    "    num_durations = 10 \n",
    "    labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "    y_train = labtrans.fit_transform(*(df_train['yy'].values, df_train['status'].values))\n",
    "elif name==\"CoxTime\":\n",
    "    labtrans = CoxTime.label_transform()\n",
    "    y_train = labtrans.fit_transform(*(df_train['yy'].values, df_train['status'].values))\n",
    "else:\n",
    "    labtrans=\"\"\n",
    "    y_train = (df_train['yy'].values, df_train['status'].values)\n",
    "\n",
    "param = pd.read_csv('model/best_param_simu/'+name+'_best_param.csv', sep = \";\",index_col=0)\n",
    "\n",
    "model,callbacks  = model_architecture.build_model(x_train, \n",
    "                           param['neurons'][rate], \n",
    "                           param['dropout'][rate], \n",
    "                           param['activation'][rate],\n",
    "                           param['lr'][rate],\n",
    "                           param['optimizer'][rate],\n",
    "                           param['n_layers'][rate],\n",
    "                           name,\n",
    "                           labtrans)\n",
    "log = model.fit(x_train, \n",
    "         y_train, \n",
    "         int(param['batch_size'][rate]), \n",
    "         epochs =100, \n",
    "         callbacks = callbacks, \n",
    "         verbose = False)\n",
    "\n",
    "_ = model.compute_baseline_hazards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We output the C-index at median time and the Integrated Brier Score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_median</th>\n",
       "      <th>ibs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566194</td>\n",
       "      <td>0.195012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c_median       ibs\n",
       "0  0.566194  0.195012"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surv = model.predict_surv_df(x_test)\n",
    "res = output_results.output_sim_data(model,surv,x_train, df_train, x_test, df_test)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
